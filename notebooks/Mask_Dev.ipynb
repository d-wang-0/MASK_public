{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMMSFtI7SOW"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9UGGdfxxRvO"
      },
      "source": [
        "## Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xKlk-43v5fq",
        "outputId": "67d3a764-44f9-45ce-d1c0-6d01729cb01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mask_public'...\n",
            "remote: Enumerating objects: 825, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 825 (delta 46), reused 65 (delta 34), pack-reused 742\u001b[K\n",
            "Receiving objects: 100% (825/825), 52.16 MiB | 24.12 MiB/s, done.\n",
            "Resolving deltas: 100% (448/448), done.\n",
            "Checking out files: 100% (79/79), done.\n",
            "/content/mask_public\n",
            "Branch 'colab' set up to track remote branch 'colab' from 'origin'.\n",
            "Switched to a new branch 'colab'\n",
            "Thu Aug 25 21:06:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/d-wang-0/mask_public.git\n",
        "%cd mask_public/\n",
        "!git checkout colab\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXlVZb9Jw5pJ"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uYnjtrisw7xl",
        "outputId": "d9f91889-a046-4a51-c9a9-204c2bef53be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sphinx_rtd_theme==0.4.*\n",
            "  Downloading sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras>=2.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: nltk>=3.4.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.7)\n",
            "Collecting numpy==1.17.0\n",
            "  Downloading numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 12.6 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.3\n",
            "  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 24.9 MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting Sphinx==1.8.5\n",
            "  Downloading Sphinx-1.8.5-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 47.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.34.0\n",
            "  Downloading tqdm-4.34.0-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow_hub==0.7.0\n",
            "  Downloading tensorflow_hub-0.7.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting attrs==19.1.0\n",
            "  Downloading attrs-19.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting python-crfsuite==0.9.6\n",
            "  Downloading python_crfsuite-0.9.6-cp37-cp37m-manylinux1_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 59.6 MB/s \n",
            "\u001b[?25hCollecting pytest==5.1.1\n",
            "  Downloading pytest-5.1.1-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 72.0 MB/s \n",
            "\u001b[?25hCollecting pylint==2.4.4\n",
            "  Downloading pylint-2.4.4-py3-none-any.whl (302 kB)\n",
            "\u001b[K     |████████████████████████████████| 302 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3->-r requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite==0.3.6->-r requirements.txt (line 6)) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite==0.3.6->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (2.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from Sphinx==1.8.5->-r requirements.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (1.47.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (0.37.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.1.1->-r requirements.txt (line 13)) (1.11.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest==5.1.1->-r requirements.txt (line 13)) (0.2.5)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.1.1->-r requirements.txt (line 13)) (8.14.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.1.1->-r requirements.txt (line 13)) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.1.1->-r requirements.txt (line 13)) (4.12.0)\n",
            "Collecting isort<5,>=4.2.5\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting astroid<2.4,>=2.3.0\n",
            "  Downloading astroid-2.3.3-py3-none-any.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "  Downloading lazy_object_proxy-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.11.2.tar.gz (27 kB)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->Sphinx==1.8.5->-r requirements.txt (line 7)) (2022.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.1.1->-r requirements.txt (line 13)) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.1.1->-r requirements.txt (line 13)) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->Sphinx==1.8.5->-r requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->Sphinx==1.8.5->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->Sphinx==1.8.5->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->Sphinx==1.8.5->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->Sphinx==1.8.5->-r requirements.txt (line 7)) (2022.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->Sphinx==1.8.5->-r requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->Sphinx==1.8.5->-r requirements.txt (line 7)) (1.1.5)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.2-cp37-cp37m-linux_x86_64.whl size=68513 sha256=05f04b107021c092975471ae4cf18441ca2583c4341f3b5d895ca03f1f991191\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/5f/62/304b411f20be41821465a82bc98baabc5e68c3cdd1eb99db71\n",
            "Successfully built wrapt\n",
            "Installing collected packages: numpy, wrapt, typed-ast, lazy-object-proxy, tqdm, tensorflow-estimator, tensorboard, Sphinx, python-crfsuite, pluggy, mccabe, keras-applications, isort, attrs, astroid, tensorflow-hub, tensorflow, sphinx-rtd-theme, sklearn-crfsuite, scikit-learn, pytest, pylint\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: Sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.1.0\n",
            "    Uninstalling attrs-22.1.0:\n",
            "      Successfully uninstalled attrs-22.1.0\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.12.0\n",
            "    Uninstalling tensorflow-hub-0.12.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.17.0 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.17.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.17.0 which is incompatible.\n",
            "spacy 3.4.1 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.34.0 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.17.0 which is incompatible.\n",
            "prophet 1.1 requires tqdm>=4.36.1, but you have tqdm 4.34.0 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.17.0 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.34.0 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.17.0 which is incompatible.\n",
            "numba 0.56.0 requires numpy<1.23,>=1.18, but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "gym 0.25.1 requires numpy>=1.18.0, but you have numpy 1.17.0 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.17.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Sphinx-1.8.5 astroid-2.3.3 attrs-19.1.0 isort-4.3.21 keras-applications-1.0.8 lazy-object-proxy-1.4.3 mccabe-0.6.1 numpy-1.17.0 pluggy-0.13.1 pylint-2.4.4 pytest-5.1.1 python-crfsuite-0.9.6 scikit-learn-0.21.3 sklearn-crfsuite-0.3.6 sphinx-rtd-theme-0.4.3 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 tensorflow-hub-0.7.0 tqdm-4.34.0 typed-ast-1.4.3 wrapt-1.11.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOfm0Fm5w-uJ",
        "outputId": "d849d621-870b-41ae-ac08-e184b87293cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "Successfully installed keras-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z54T9O4WxA2g",
        "outputId": "38be27c1-5bc3-4981-ed9e-e15fcbba3f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.17.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.47.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_gpu==1.14\n",
            "  Downloading tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1 MB 9.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.47.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (0.5.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.2.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow_gpu==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow_gpu==1.14) (1.5.2)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install tensorflow_gpu==1.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuLsgv9bxBKQ",
        "outputId": "68f91e2d-b3e1-40cb-9334-ab0a90a73452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10) (1.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed h5py-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py==2.10\n",
        "# fix loading model? https://stackoverflow.com/questions/67236747/str-object-has-no-attribute-decode-for-tensorflow-in-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTjVh31SxDIY",
        "outputId": "065ac032-7cc6-49e9-842f-d0b58b7aefd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.21.3)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7xDKND99E5G",
        "outputId": "ca5a2f9a-3417-4049-9fa6-b7d69a3a321b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.34.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-eK6gSL9HnJ"
      },
      "source": [
        "## Download large files (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueWie6vK9N48",
        "outputId": "b8b0e82e-267f-4cfe-dc63-84e7929e9808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1H_9jDgBlME0Xcb8Fp4M38l_Zdc4feHlH\n",
            "To: /content/mask_public/Models/NER_BiLSTM_ELMo_i2b2_pad_mask.h5\n",
            "100% 151M/151M [00:01<00:00, 132MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XdZPz4sTvLx6g_PXKK0_37o_JsavbAZQ\n",
            "To: /content/mask_public/Models/NER_BiLSTM_ELMo_i2b2.h5\n",
            "100% 151M/151M [00:01<00:00, 88.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download some weights\n",
        "!gdown 1H_9jDgBlME0Xcb8Fp4M38l_Zdc4feHlH -O Models/\n",
        "!gdown 1XdZPz4sTvLx6g_PXKK0_37o_JsavbAZQ -O Models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Dataset\n",
        "1. Download `2014_training-PHI-Gold-Set1.tar.gz` and `training-PHI-Gold-Set2.tar.gz` from the i2b2 dataset\n",
        "2. Upload them to google drive\n",
        "3. Share the files on drive so \"Anyone on the Internet with the link can view\"\n",
        "4. Paste links to the shared files here and it will be extracted and placed in the correct locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Va3IY8YE9-50",
        "outputId": "1ca6b89e-75f8-4c00-e6ef-ce878f0272c0"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "url1 = \"\" #@param {type:\"string\"}\n",
        "gdown.download(url1, \"set1.tar.gz\", fuzzy=True)\n",
        "url2 = \"\" #@param {type:\"string\"}\n",
        "gdown.download(url2, \"set2.tar.gz\", fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "42phtvXX_LH1"
      },
      "outputs": [],
      "source": [
        "!rm dataset/input/*\n",
        "!tar -xf set1.tar.gz --strip=1 --directory dataset/input/ training-PHI-Gold-Set1/\n",
        "!tar -xf set2.tar.gz --strip=1 --directory dataset/input/ training-PHI-Gold-Set2/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdyr41aRxQ3Z"
      },
      "source": [
        "## Setup Google Drive (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0fiY2HgxVFI",
        "outputId": "bfeb9474-0216-4fbb-a7a6-673f0fa95bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPF6qrnM0Qot"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PMHLz7EubIK"
      },
      "source": [
        "## Parsing i2b2 data\n",
        "\n",
        "From `utils\\readers.py`\n",
        "\n",
        "The i2b2 dataset is in `.xml` format.\n",
        "\n",
        "The text of the document is contained under the `<TEXT>` tag.\n",
        "\n",
        "Under the `<TAGS>` tag, there are child elements containing tagged categories. \n",
        "\n",
        "For example:\n",
        "\n",
        "`<NAME id=\"P0\" start=\"1601\" end=\"1615\" text=\"Luke Skywalker\" TYPE=\"PATIENT\" comment=\"\" />`\n",
        "\n",
        "* `id`: appears to be `P{index of the tag`}`\n",
        "* `start`,`end`: the starting and ending index of the text\n",
        "* `text`: the tagged text\n",
        "* `TYPE`: seems to be a more detailed category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FIp6EXYZubi6"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import xml.etree.ElementTree as ET\n",
        "from os.path import isfile, join\n",
        "\n",
        "def read_i2b2_file(path):\n",
        "    \"\"\"\n",
        "    Function that reads i2b2 files from path and returns a list of documents with text and tags\n",
        "    :param path: Path where the files are located\n",
        "    :return: list of documents containing dictionary {\"id\":file,\"text\":text,\"tags\":document_tags}\n",
        "    \"\"\"\n",
        "    tree = ET.parse(path)\n",
        "    root = tree.getroot()\n",
        "    document_tags = []\n",
        "    for child in root:\n",
        "        if child.tag == \"TEXT\":\n",
        "            text = child.text\n",
        "        if child.tag == \"TAGS\":\n",
        "            for chch in child:\n",
        "                tag = chch.tag\n",
        "                attributes = chch.attrib\n",
        "                start = attributes[\"start\"]\n",
        "                end = attributes[\"end\"]\n",
        "                content = attributes[\"text\"]\n",
        "                type= attributes[\"TYPE\"]\n",
        "                document_tags.append({\"tag\":tag,\"start\":start,\"end\":end,\"text\":content,\"type\":type})\n",
        "    return {\"id\":path,\"text\":text,\"tags\":document_tags}\n",
        "\n",
        "def read_i2b2_data(path):\n",
        "    \"\"\"\n",
        "    Function that reads i2b2 files from path and returns a list of documents with text and tags\n",
        "    :param path: Path where the files are located\n",
        "    :return: list of documents containing dictionary {\"id\":file,\"text\":text,\"tags\":document_tags}\n",
        "    \"\"\"\n",
        "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "    documents = []\n",
        "    for file in onlyfiles:\n",
        "        documents.append(read_i2b2_file(join(path,file)))\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAFErJhS0lq8",
        "outputId": "03f85987-7740-4063-fc2b-4f5a59c3662c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 790 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import importlib\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "\n",
        "#@markdown ### Enter path to data:\n",
        "path = \"dataset/input/\" #@param {type:\"string\"}\n",
        "documents = read_i2b2_data(path)\n",
        "print(f'Loaded {len(documents)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77WuvkyL3M4N"
      },
      "source": [
        "## Tokenize\n",
        "\n",
        "From `utils\\spec_tokenizers.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iglixk_rwLnL",
        "outputId": "8d40b424-0604-4d3b-8174-1f3ca8e1c470"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#Code by Nikola Milosevic\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize.util import align_tokens\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import re\n",
        "import tensorflow_hub as hub\n",
        "#from bert.tokenization import FullTokenizer\n",
        "import tensorflow as tf\n",
        "sess = tf.compat.v1.Session()\n",
        "_treebank_word_tokenizer = TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul3AiQ5d05ul"
      },
      "source": [
        "First we tokenize into sentences using `nltk.sent_tokenize` and then tokenize sentences using `_treebank_word_tokenizer.tokenize`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LE4AcE_L09B4"
      },
      "outputs": [],
      "source": [
        "def custom_word_tokenize(text, language='english', preserve_line=False):\n",
        "    \"\"\"\n",
        "    Return a tokenized copy of *text*,\n",
        "    using NLTK's recommended word tokenizer\n",
        "    (currently an improved :class:`.TreebankWordTokenizer`\n",
        "    along with :class:`.PunktSentenceTokenizer`\n",
        "    for the specified language).\n",
        "\n",
        "    :param text: text to split into words\n",
        "    :param text: str\n",
        "    :param language: the model name in the Punkt corpus\n",
        "    :type language: str\n",
        "    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
        "    :type preserver_line: bool\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    sentences = [text] if preserve_line else nltk.sent_tokenize(text, language) # tokenize sentences\n",
        "    # for some reason, these tokenized sentences are ignored later on\n",
        "    for sent in sentences:\n",
        "        for token in _treebank_word_tokenizer.tokenize(sent):\n",
        "            if \"-\" in token:\n",
        "                m = re.compile(\"(\\d+)(-)([a-zA-z-]+)\") # not sure exactly what this is supposed to match but split these up\n",
        "                g = m.match(token)\n",
        "                if g:\n",
        "                    for group in g.groups():\n",
        "                        tokens.append(group)\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5bcdxYn1CtV"
      },
      "source": [
        "We then need to find the locations of these tokens in the original text so we can match the tokens with the tagged categories from the i2b2 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3D4xloiY1DBp"
      },
      "outputs": [],
      "source": [
        "def custom_span_tokenize(text, language='english', preserve_line=True):\n",
        "    \"\"\"\n",
        "            Returns a spans of tokens in text.\n",
        "\n",
        "            :param text: text to split into words\n",
        "            :param language: the model name in the Punkt corpus\n",
        "            :type language: str\n",
        "            :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
        "            :type preserver_line: bool\n",
        "            \"\"\"\n",
        "    tokens = custom_word_tokenize(text)\n",
        "    tokens = ['\"' if tok in ['``', \"''\"] else tok for tok in tokens] # standardize quotation marks\n",
        "    return align_tokens(tokens, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WFuiajl1nb2"
      },
      "source": [
        "We need to tag the appropriate tokens with their categories.\n",
        "\n",
        "The tokens are split into sequences based on punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LYCPyBpi1eiw"
      },
      "outputs": [],
      "source": [
        "def tokenize_to_seq(documents):\n",
        "    sequences = []\n",
        "    sequence = []\n",
        "    for doc in documents:\n",
        "        if len(sequence)>0:\n",
        "            sequences.append(sequence)\n",
        "        sequence = []\n",
        "        text = doc[\"text\"]\n",
        "        file = doc[\"id\"]\n",
        "        text = text.replace(\"\\\"\", \"'\")\n",
        "        text = text.replace(\"`\", \"'\")\n",
        "        text = text.replace(\"``\", \"\")\n",
        "        text = text.replace(\"''\", \"\")\n",
        "        tokens = custom_span_tokenize(text)\n",
        "        for token in tokens:\n",
        "            token_txt = text[token[0]:token[1]]\n",
        "            found = False\n",
        "            for tag in doc[\"tags\"]:\n",
        "                if int(tag[\"start\"])<=token[0] and int(tag[\"end\"])>=token[1]:\n",
        "                    token_tag = tag[\"tag\"]\n",
        "                    #token_tag_type = tag[\"type\"]\n",
        "                    found = True\n",
        "            if found==False:\n",
        "                token_tag = \"O\"\n",
        "                #token_tag_type = \"O\"\n",
        "            sequence.append((token_txt,token_tag))\n",
        "            if token_txt == \".\" or token_txt == \"?\" or token_txt == \"!\":\n",
        "                sequences.append(sequence)\n",
        "                sequence = []\n",
        "        sequences.append(sequence)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyUA2Kqs2W-8"
      },
      "source": [
        "This function is used for tokenizing text we don't have any tags for, such as test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s7FCMFt42gjr"
      },
      "outputs": [],
      "source": [
        "def tokenize_fa(documents):\n",
        "    \"\"\"\n",
        "              Tokenization function. Returns list of sequences\n",
        "\n",
        "              :param documents: list of texts\n",
        "              :type language: list\n",
        "\n",
        "              \"\"\"\n",
        "    sequences = []\n",
        "    sequence = []\n",
        "    for doc in documents:\n",
        "        if len(sequence) > 0:\n",
        "            sequences.append(sequence)\n",
        "        sequence = []\n",
        "        text = doc\n",
        "        text = text.replace(\"\\\"\", \"'\")\n",
        "        text = text.replace(\"`\", \"'\")\n",
        "        text = text.replace(\"``\", \"\")\n",
        "        text = text.replace(\"''\", \"\")\n",
        "        tokens = custom_span_tokenize(text)\n",
        "        for token in tokens:\n",
        "            token_txt = text[token[0]:token[1]]\n",
        "            found = False\n",
        "            if found == False:\n",
        "                token_tag = \"O\"\n",
        "                # token_tag_type = \"O\"\n",
        "            sequence.append((token_txt, token_tag))\n",
        "            if token_txt == \".\" or token_txt == \"?\" or token_txt == \"!\":\n",
        "                sequences.append(sequence)\n",
        "                sequence = []\n",
        "        sequences.append(sequence)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFAkyBRA20eY",
        "outputId": "133923b7-7cc6-4fbc-ffa3-3f1f5c21b951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 34643 sequences\n"
          ]
        }
      ],
      "source": [
        "tokens_labels = tokenize_to_seq(documents)\n",
        "print(f'Split into {len(tokens_labels)} sequences')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK6Zl4LX6sIc"
      },
      "source": [
        "### Tokenization Testing\n",
        "Enter some text to see how it gets tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU_qtKcK60EZ",
        "outputId": "6d683f39-81eb-49e3-937a-bbe664357937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['All', 'humans', 'are', 'mortal', '.']\n",
            "['Socrates', 'is', 'human', '.']\n",
            "['Hence', ',', 'Socrates', 'is', 'mortal', '.']\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### Enter text to tokenize:\n",
        "text = \"All humans are mortal. Socrates is human. Hence, Socrates is mortal.\" #@param {type:\"string\"}\n",
        "for seq in tokenize_fa([text]):\n",
        "    print([word for word, tag in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vJTCG1pMZFI4"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Sort all tagged text\n",
        "full_tag_examples = {}\n",
        "for document in documents:\n",
        "    for tag in document['tags']:\n",
        "        if tag['tag'] not in full_tag_examples:\n",
        "            full_tag_examples[tag['tag']] = [tag['text']]\n",
        "        full_tag_examples[tag['tag']].append(tag['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5BQ_rw7dRT"
      },
      "source": [
        "# Split and Augment data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgPF-ml070vA"
      },
      "source": [
        "## Transform Sequences\n",
        "\n",
        "`tokenize_to_seq` returns a list of tuples of `({text},{tag})`. To pass this into a model, we need to process the list into `X`(the sequences) and `Y` (the labels)\n",
        "\n",
        "The exact nature of this processing will depend on the model itself so in the original code, each model object has it's own `transform_sequences` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TUh0Yqf18GAK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "PADWORD = \"PADword\"\n",
        "def transform_sequences(token_sequences, max_len=50):\n",
        "    X = []\n",
        "    Y = []\n",
        "    all_tags = []\n",
        "    for tok_seq in token_sequences:\n",
        "        X_seq = []\n",
        "        Y_seq = []\n",
        "        for i in range(0, max_len):\n",
        "            try:\n",
        "                X_seq.append(tok_seq[i][0])\n",
        "                Y_seq.append(tok_seq[i][1])\n",
        "                all_tags.append(tok_seq[i][1])\n",
        "            except:\n",
        "                X_seq.append(PADWORD)\n",
        "                Y_seq.append(\"O\")\n",
        "        X.append(X_seq)\n",
        "        Y.append(Y_seq)\n",
        "    tags2index = {'O': 0, 'ID': 1, 'PHI': 2, 'NAME': 3, 'CONTACT': 4,\n",
        "                    'DATE': 5, 'AGE': 6, 'PROFESSION': 7, 'LOCATION': 8}\n",
        "\n",
        "    Y = [[tags2index[w] for w in s] for s in Y]\n",
        "\n",
        "    return X, Y\n",
        "max_len = 50 #@param {type:\"integer\"}\n",
        "X,Y = transform_sequences(tokens_labels, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vEoZO737fN"
      },
      "source": [
        "Next, we need to split the sequences into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VMfCXDkM379v"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Enter percent size of test set (default: 0.2):\n",
        "test_size = 0.2 #@param {type:\"slider\", min:0.0, max:1, step:0.1}\n",
        "#@markdown ### Enter random seed (default: 42):\n",
        "random_state = 42 #@param {type:\"integer\"}\n",
        "X_train,X_test, Y_train,Y_test = train_test_split(X,Y,test_size=test_size,random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSB_UNMb4NBh"
      },
      "source": [
        "Some more pre-processing needs to be done for training. This is originally found in the model's `train` method.\n",
        "\n",
        "For `NER_BiLSTM_ELMo_i2b2_pad_mask`, the sequences need to be the same length so we need to do padding/truncation. This also reserves the last 10% of the data for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EjTcvH8B4uTC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def pre_data(X,Y, max_len):\n",
        "    # X = pad_sequences(\n",
        "    #     X,\n",
        "    #     maxlen=max_len,\n",
        "    #     dtype=object,\n",
        "    #     padding='post',\n",
        "    #     truncating='post',\n",
        "    #     value=PADWORD\n",
        "    # )\n",
        "    # Y = pad_sequences(\n",
        "    #     Y,\n",
        "    #     maxlen=max_len,\n",
        "    #     dtype='int',\n",
        "    #     padding='post',\n",
        "    #     truncating='post',\n",
        "    #     value=0\n",
        "    # )\n",
        "    first = int(np.floor(0.9*len(X)))\n",
        "    X_tr, X_val = X[:first], X[first:]\n",
        "    y_tr, y_val = Y[:first], Y[first:]\n",
        "    y_tr = np.array(y_tr)\n",
        "    y_val = np.array(y_val)\n",
        "    y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "    y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
        "    X_tr = np.array(X_tr)\n",
        "    y_tr = np.array(y_tr)\n",
        "    seq_lens_tr = np.array([max_len]*len(X_tr))\n",
        "    X_val = np.array(X_val)\n",
        "    seq_lens_val = np.array([max_len]*len(X_val))\n",
        "    return X_tr,seq_lens_tr, y_tr, X_val, seq_lens_val, y_val\n",
        "max_len = 100 #@param {type:\"integer\"}\n",
        "X_tr, seq_lens_tr, y_tr, X_val, seq_lens_val, y_val = pre_data(X_train, Y_train, max_len=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKIVDWQm32tZ"
      },
      "source": [
        "# Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H9do6_Am35K5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True, name=\"{}_module\".format(\"elmo\"))\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-GT2OxE4URB"
      },
      "source": [
        "## ELMo Embedding Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FQbddZw53-p1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "class ElmoEmbeddingLayer(Layer):\n",
        "    def __init__(self, elmo_model, **kwargs):\n",
        "        self.dimensions = 1024\n",
        "        self.trainable=True\n",
        "        self.elmo = elmo_model\n",
        "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
        "        #                        name=\"{}_module\".format(self.name))\n",
        "\n",
        "        self._trainable_weights += tf.trainable_variables(scope=\"^{}_module/.*\".format(\"elmo\"))\n",
        "        # sess.run(tf.global_variables_initializer())\n",
        "        # sess.run(tf.tables_initializer())\n",
        "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        result = self.elmo(inputs = {\n",
        "            \"tokens\": inputs[0],\n",
        "            \"sequence_len\": inputs[1]\n",
        "        },\n",
        "                      as_dict=True,\n",
        "                      signature='tokens',\n",
        "                      )['elmo']\n",
        "        return result\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs[0], 'PADWORD')\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcxfYjIq7zEj"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjK1-Kkn4Apo",
        "outputId": "4a6938b9-6591-4678-cbe0-b71bf755a139"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:Entity <bound method ElmoEmbeddingLayer.call of <__main__.ElmoEmbeddingLayer object at 0x7efb69102450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ElmoEmbeddingLayer.call of <__main__.ElmoEmbeddingLayer object at 0x7efb69102450>>: AttributeError: module 'gast' has no attribute 'Index'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method ElmoEmbeddingLayer.call of <__main__.ElmoEmbeddingLayer object at 0x7efb69102450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ElmoEmbeddingLayer.call of <__main__.ElmoEmbeddingLayer object at 0x7efb69102450>>: AttributeError: module 'gast' has no attribute 'Index'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "tokens (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "seq_len (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "elmo_embedding_layer (ElmoEmbed (None, None, 1024)   4           tokens[0][0]                     \n",
            "                                                                 seq_len[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, None, 1024)   6295552     elmo_embedding_layer[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, None, 1024)   6295552     bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, 1024)   0           bidirectional[0][0]              \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 9)      9225        add[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 12,600,333\n",
            "Trainable params: 12,600,333\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Lambda, Input, Masking\n",
        "from utils.spec_tokenizers import tokenize_fa\n",
        "n_tags = 9\n",
        "\n",
        "input_text = Input(shape=(None,), dtype=\"string\", name='tokens')\n",
        "input_len = Input(shape=[], dtype=tf.int32, name='seq_len')\n",
        "embedding = ElmoEmbeddingLayer(elmo_model)([input_text, input_len])\n",
        "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                        recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                            recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
        "model = Model(inputs=[input_text, input_len], outputs=out)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilLR-erd72_4"
      },
      "source": [
        "## Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jidxU8I253Q6"
      },
      "outputs": [],
      "source": [
        "#@title ## Load weights (Optional)\n",
        "#@markdown ---\n",
        "load_weights = True #@param {type:\"boolean\"}\n",
        "#@markdown ### Enter path to `.h5` file containing model weights:\n",
        "# Drive link /content/gdrive/My Drive/Mask/NER_BiLSTM_ELMo_i2b2_pad_mask.h5\n",
        "weights_path = 'Models/NER_BiLSTM_ELMo_i2b2_pad_mask.h5' #@param {type:\"string\"}\n",
        "if load_weights and weights_path:\n",
        "    model.load_weights(weights_path)\n",
        "#@markdown To use files from Google Drive, append `/content/gdrive/My Drive/` to file path in Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHM7OfNN74ZC"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U6012lz43Or"
      },
      "outputs": [],
      "source": [
        "epochs = 5 #@param {type:\"integer\"}\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "model.fit(x = [X_tr, seq_lens_tr], y = y_tr,validation_data=([X_val, seq_lens_val], y_val),batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RtyHwls5ZOJ"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQyuGQry6TyX"
      },
      "source": [
        "## Basic Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6FsdSczI5aWC"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, X, Y):\n",
        "    from sklearn import metrics\n",
        "    seq_lens = np.array([len(s) for s in X])\n",
        "    Y_pred = model.predict([np.array(X), seq_lens])\n",
        "    from sklearn import metrics\n",
        "    index2tags = {0: 'O', 1: 'ID', 2: 'PHI', 3: 'NAME', 4: 'CONTACT',\n",
        "                    5: 'DATE', 6: 'AGE', 7: 'PROFESSION', 8: 'LOCATION'}\n",
        "    labels = [\"O\", \"ID\", \"PHI\", \"NAME\", \"CONTACT\", \"DATE\", \"AGE\",\n",
        "                \"PROFESSION\", \"LOCATION\"]\n",
        "    Y_pred_F = np.argmax(Y_pred, axis=2).flatten()\n",
        "    Y_pred_F = [index2tags[i] for i in Y_pred_F]\n",
        "    Y_test_F = np.array(Y).flatten()\n",
        "    Y_test_F = [index2tags[i] for i in Y_test_F]\n",
        "    print(metrics.classification_report(Y_test_F, Y_pred_F, labels=labels))\n",
        "    print(metrics.classification_report(Y_test_F, Y_pred_F, labels=labels[1:]))\n",
        "    from matplotlib import pyplot as plt\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    metrics.ConfusionMatrixDisplay.from_predictions(y_true = Y_test_F, y_pred = Y_pred_F, labels=labels, normalize='true', xticks_rotation='vertical',ax=ax)\n",
        "    return np.argmax(Y_pred, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVpHHLVg5z9r"
      },
      "outputs": [],
      "source": [
        "Y_pred = evaluate(model, X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keU9XFzA6VtJ"
      },
      "source": [
        "## Check wrong samples\n",
        "\n",
        "Compile a list of samples that were incorrectly labeled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQqmPPOO6ZQg"
      },
      "outputs": [],
      "source": [
        "errors = Y_pred==np.array(Y_test[:10])\n",
        "sample_errors = np.all(errors,axis=1)\n",
        "wrong_samples = []\n",
        "for i, sample in enumerate(sample_errors):\n",
        "    if not sample:\n",
        "        sample = X_val[i]\n",
        "        pred_labels = Y_pred[i]\n",
        "        true_labels = Y_test[i]\n",
        "        words = []\n",
        "        for j, word in enumerate(sample):\n",
        "            if word != PADWORD:\n",
        "                words.append((word, true_labels[j], pred_labels[j]))\n",
        "        wrong_samples.append(words)\n",
        "def has_name(sample):\n",
        "    for word in sample:\n",
        "        if word[2] != word[1] and (word[1] == 3 or word[2] == 3 or word[1] == 7 or word[2] == 7):\n",
        "            return True\n",
        "wrong_name = [sample for sample in wrong_samples if has_name(sample)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln0m8ntWldNl"
      },
      "outputs": [],
      "source": [
        "wrong_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oagoz_m4yxjf"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mhZZS2d3zh2"
      },
      "outputs": [],
      "source": [
        "#@title ## Save weights (Optional)\n",
        "#@markdown ---\n",
        "#@markdown ### Enter path to `.h5` file to save weights:\n",
        "#@markdown **Existing files will be overwritten.**\n",
        "weights_path = '' #@param {type:\"string\"}\n",
        "if weights_path:\n",
        "    model.save_weights(weights_path)\n",
        "#@markdown To use files from Google Drive, append `/content/gdrive/My Drive/` to file path in Google Drive."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yXlVZb9Jw5pJ",
        "Xdyr41aRxQ3Z"
      ],
      "name": "Mask Dev V2",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c426d3892c2057fc51860d649aeb6768667ef17e7800411a57a592956e9f27a2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
